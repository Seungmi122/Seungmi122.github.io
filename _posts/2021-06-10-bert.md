---
title: "[Paper Review] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
excerpt: ""
date: 2021-06-10 22:000 -0400
use_math: true
tags :
- BERT
- pre-trained model

category: [NLP]









---







## Reference 

- https://arxiv.org/pdf/1810.04805.pdf