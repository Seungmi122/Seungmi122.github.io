---
title: "[Paper Review] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
excerpt: "BERT"
date: 2021-06-10 22:000 -0400
use_math: true
tags :
- BERT
- pre-trained model

category: [NLP]









---



BERT consist of two parts: **Pre-training and Fine-Tuning**

![img](/Users/seungmi/workspace/studying/Seungmi122.github.io/assets/2021-06-11-bert1.png)



## Pre-training

### 1. Mask LM (MLM)

### 2. Next Sentence Prediction (NSP)





## Fine-Tuning









## Reference 

- https://arxiv.org/pdf/1810.04805.pdf