---
title: "[Paper Review] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
excerpt: "BERT"
date: 2021-06-10 22:000 -0400
use_math: true
tags :
- BERT
- pre-trained model

category: [NLP]









---





Self-attention. 중간에 랜덤하게 어떤 단어들을 가림으로써 그 단어들을 예측하는거. Masked 보단 unnatural 해서 생성에는 힘들겠지만 classification 이나 다른 task 에서는 오히려 더 많이 trained 되어있기 때문에 더 잘 나오기도





BERT consist of two parts: **Pre-training and Fine-Tuning**

![img](/Users/seungmi/workspace/studying/Seungmi122.github.io/assets/2021-06-11-bert1.png)



## Pre-training

### 1. Mask LM (MLM)

### 2. Next Sentence Prediction (NSP)





## Fine-Tuning









## Reference 

- https://arxiv.org/pdf/1810.04805.pdf