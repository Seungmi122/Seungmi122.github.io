---
title: "[Paper Review] Deep contextualized word representations (ELMo)"
excerpt: "ELMo"
date: 2021-06-10 22:000 -0400
use_math: true
tags :
- language model
- word representation


category: [NLP]








---



# Contextual Representation 

**Static representation** such as *Word2Vec, Glove* considers a word as a single vector with a single fixed meaning.

However, in many cases, a word has different meanings depending on its syntax, semantics and context, so  that we are needed to learn its **dynamic** representation. 

--> **Contextual representation** such as **ELMo, BERT, GPT** thinks that a single word can have *multiple vectors*



![img](/Users/seungmi/workspace/studying/Seungmi122.github.io/assets/2021-06-10-elmo1.png)



# ELMo: Embeddings from Language Models

use vectors from bi-directional LSTM trained with language models on a large text corpus

How it works?

*Language Modeling* 



# Bidirectional language models

왜 backward. 가 필요한가? practical env 에서는 의미가 없어

근데 목적이 language model 잘 학습시키자가 아니라

학습을 위한 학습!을 위해 쓰이는거기 때문에, LM 이 목적이 아니라 수단







## Reference

- https://arxiv.org/abs/1802.05365
- https://wikidocs.net/33930
- http://jalammar.github.io/illustrated-bert/