---
title: "[Paper Review] Deep contextualized word representations"
excerpt: "ELMo"
date: 2021-06-10 22:000 -0400
use_math: true
tags :
- language model
- word representation


category: [NLP]








---



Static representation such as *Word2Vec, Glove* considers a word as a single vector with a single fixed meaning.

Contextual representation such as *ELMo, BERT, GPT* thinks that a single word can have multiple vectors with different meanings depending on its context. The more complex a word is, the more we are needed to learn its **dynamic** representation.





