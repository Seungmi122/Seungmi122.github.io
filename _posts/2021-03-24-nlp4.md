---
title: "[Paper Review] Sequence to Sequence Learning with Neural Networks"
excerpt: "Briefly introduce what Seq2seq is"
date: 2021-03-24 14:000 -0400
author : 오승미
use_math: true
tags :
- seq2seq
- paper review

category: [NLP]


---



Today, we are going to cover [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf), 2014.

Traditional DNNs improved the fields of machine translation, speech recognition ,etc remarkably. However, the traditonal DNNs can only handle fixed dimension of input and outputs, which is a critical flaw, especially in translating a sequence where the input and output dimensions are unknown.

## Architecture

Let's look at **Seq2Seq** architecture.

![Screen Shot 2021-03-25 at 1.00.38 AM](/assets/2021-03-24-nlp4.png)

It seems like one long RNN but they use two LSTM as the encoder and decoder respectively. In this example, the model takes a sentence "ABC" as an input and translate into "XYZ" as an output. The encoder reads "A", "B", "C" sequentially then the encoder creates a large fixed-dimensional vector. When "<EOS>", meaning the end of sentence is read, the encoder can tell it is the end of the input sentence then the decoder extracts the output sentence from the fixed vector.

`Why LSTM?`

Compared to tradional RNNs, LSTMs can reflect the long-term dependency.



## Three findings of this paper:

- Two different LSTMs

- Deep LSTMs significantly outperformed shallow LSTMs

- Reversing the order of the words improved the model performance



Well, seq2seq model seems to be overwhelmed by [Transformer](https://arxiv.org/abs/1706.03762) and [this post](https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c) might be interesting if you are wondering the future of seq2seq model.

# Reference

- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)
