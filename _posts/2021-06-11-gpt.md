---
title: "[Paper Review] Language Models are Unsupervised Multitask Learners (GPT-2)"
excerpt: "GPT-2"
date: 2021-06-11 20:000 -0400
use_math: true
tags :
- gpt2

category: [NLP]



---



# Model Architecture

![Screen Shot 2021-06-12 at 3.30.38 AM](/assets/2021-06-11-gpt.png)

- Multiple layers of transfomer decoders 

- Similar looking with Transformer, BERT

  



# Masked Self-Attention

![Screen Shot 2021-06-12 at 3.31.25 AM](/assets/2021-06-11-gpt2.png)![Screen Shot 2021-06-12 at 3.48.49 AM](/assets/2021-06-11-gpt4.png)

- Unlike self-attention,

- Mask out words on the right side of our target word

- due to its structure, we also call it **auto-regressive model**: predict next word step by step

- this seems more natural since we have no idea what next words will be in practical problems

- leading to improve model performance in generative tasks

  





## How GPT-2 can work on multiple tasks without fine-tuning?

- Both original input and 'task' as an input
- Ex. "how are you" + translate to Korean
- Let our model know the task for better model performance

 

-----

## GPT vs BERT

GPT:

- Pre-trained to predict the next word
- Uni-directional
- good at generating tasks
- layers of decoders only



BERT:

- Mask out the word in the middle and predict it
- Bi-directional
- good at understanding its meaning 
- layers of encoders only



Suppose that we want to predict "거기" in the below sentence.

![Screen Shot 2021-06-12 at 3.41.18 AM](/assets/2021-06-11-gpt3.png)

GPT only refers to "어제, 카페, 갔었어" (not in sequetial manner though), while masking out all tokens coming up after our target word. On the other hand, BERT uses all the tokens.

In both models, however, basically atttention mechanism is implemented, being trained to learn "important" word in the candiate pool via updating the whole model.

## Reference

- [original paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

- [Youtube](https://www.youtube.com/watch?v=3n6157XNYyw)
- [ratsgo's NLPBOOK](https://ratsgo.github.io/nlpbook/docs/language_model/bert_gpt/)

