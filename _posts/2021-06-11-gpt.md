---
title: "OpenAI-GPT"
excerpt: "GPT"
date: 2021-06-11 23:000 -0400
use_math: true
tags :
- gpt

category: [NLP]









---





아키텍처상 decoder 만 사용



masked self-attention: 다음단어를 모르는 상태에서 다음단어를 예측하는거. more natural

Self-attention. 중간에 랜덤하게 어떤 단어들을 가림으로써 그 단어들을 예측하는거. Masked 보단 unnatural 해서 생성에는 힘들겠지만 classification 이나 다른 task 에서는 오히려 더 많이 trained 되어있기 때문에 더 잘 나오기도

기본 매커니즘은 사실 비슷.

 







## Reference

- 

